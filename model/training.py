import json
import torch
import warnings
from transformers import RobertaTokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq
from datasets import Dataset

warnings.filterwarnings("ignore")

def main():
    # æª¢æŸ¥ä¸¦è¨­å®š GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    if torch.cuda.is_available():
        print(f"GPU åç¨±: {torch.cuda.get_device_name(0)}")
        print(f"GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("è­¦å‘Šï¼šæœªåµæ¸¬åˆ° GPUï¼Œå°‡ä½¿ç”¨ CPU è¨“ç·´")

    # åˆå§‹åŒ– tokenizer å’Œæ¨¡å‹
    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')
    model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base')
    model = model.to(device)

    # æŒ‡å®šè³‡æ–™è·¯å¾‘
    json_file_path = './Tavernari-git-commit-message-dt/Tavernari-git-commit-message-dt_commit_message.json'

    # è®€å– JSON æª”æ¡ˆ
    with open(json_file_path, 'r', encoding='utf-8') as f:
        training_data = json.load(f)

    print(f"ğŸ“Š åŸå§‹è³‡æ–™: {len(training_data)} ç­†")

    # é¡¯ç¤ºå‰å¹¾ç­†è³‡æ–™
    for i, item in enumerate(training_data[:3]):
        print(f"ç¬¬{i+1}ç­†:")
        print(f"  Input: {item['input'][:100]}...")
        print(f"  Output: {item['output']}")

    # è¨»è§£è™•ç†å‡½æ•¸
    def handle_comments(diff):
        lines = diff.split('\n')
        processed_lines = [line for line in lines if not line.strip().startswith('//')]
        return '\n'.join(processed_lines)

    def preprocess(examples):
        """å®Œå…¨ä¿®æ­£çš„é è™•ç†å‡½æ•¸"""
        
        # è™•ç† input
        processed_inputs = [handle_comments(item) for item in examples["input"]]
        
        # Tokenize inputs
        model_inputs = tokenizer(
            processed_inputs,
            max_length=256,
            truncation=True,
            padding=True
        )
        
        # Tokenize targets
        labels = tokenizer(
            examples["output"],
            max_length=64,
            truncation=True,
            padding=True
        )
        
        # è¨­å®š labels
        label_ids = []
        for label_seq in labels["input_ids"]:
            label_with_ignore = [
                -100 if token == tokenizer.pad_token_id else token 
                for token in label_seq
            ]
            label_ids.append(label_with_ignore)
        
        model_inputs["labels"] = label_ids
        return model_inputs

    # å»ºç«‹è³‡æ–™é›†
    print(f"\nğŸ”„ é è™•ç†è³‡æ–™...")
    dataset = Dataset.from_list(training_data).map(preprocess, batched=True)
    
    # æª¢æŸ¥ç¬¬ä¸€ç­†è™•ç†å¾Œçš„è³‡æ–™
    sample = dataset[0]
    print(f"\nğŸ” ç¬¬ä¸€ç­†è™•ç†å¾Œçš„è³‡æ–™:")
    print(f"Input IDs length: {len(sample['input_ids'])}")
    print(f"Labels length: {len(sample['labels'])}")
    print(f"Input text: {tokenizer.decode(sample['input_ids'], skip_special_tokens=True)[:100]}...")
    
    # è§£ç¢¼ labelsï¼ˆæ’é™¤ -100ï¼‰
    valid_labels = [t for t in sample['labels'] if t != -100]
    print(f"Label text: {tokenizer.decode(valid_labels, skip_special_tokens=True)}")

    # è¨“ç·´è¨­å®š
    training_args = TrainingArguments(
        output_dir="./commit-model",
        num_train_epochs=2,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        save_steps=50,
        logging_steps=5,
        save_total_limit=2,
        load_best_model_at_end=False,
        
        # å­¸ç¿’ç‡è¨­å®š
        learning_rate=5e-5,
        warmup_steps=10,
        
        # è³‡æ–™è¼‰å…¥è¨­å®š
        dataloader_num_workers=0,
        dataloader_pin_memory=False,
        
        # GPU å„ªåŒ–
        fp16=torch.cuda.is_available(),
        no_cuda=False,
        
        # å…¶ä»–è¨­å®š
        weight_decay=0.01,
        logging_dir='./logs',
        report_to=None,
    )

    # è³‡æ–™æ•´ç†å™¨
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        model=model,
        padding=True
    )

    # å»ºç«‹è¨“ç·´å™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator
    )

    # é–‹å§‹è¨“ç·´
    print(f"\nğŸš€ é–‹å§‹è¨“ç·´...")
    print(f"è³‡æ–™é›†å¤§å°: {len(training_data)} ç­†")
    print(f"æ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size}")
    print(f"æ¢¯åº¦ç´¯ç©: {training_args.gradient_accumulation_steps}")
    print(f"æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")

    # è¨“ç·´
    train_result = trainer.train()
    
    print(f"\nğŸ“Š è¨“ç·´çµæœ:")
    print(f"æœ€çµ‚ Loss: {train_result.training_loss:.4f}")

    # ä¿å­˜æ¨¡å‹
    trainer.save_model("./commit-model")
    tokenizer.save_pretrained("./commit-model")

    print(f"\nâœ… è¨“ç·´å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ./commit-model")

if __name__ == '__main__':
    main()