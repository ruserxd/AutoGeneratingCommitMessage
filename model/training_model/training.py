import os
import time
import glob
from datetime import datetime

# è¨­ç½® multiprocessing å•Ÿå‹•æ–¹æ³•
import multiprocessing as mp
try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass

import torch
import json
import random
import logging
import wandb
from datasets import Dataset
from transformers import (
    T5ForConditionalGeneration,
    RobertaTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
)

# ç¦ç”¨ tokenizers ä¸¦è¡Œè™•ç†
os.environ["TOKENIZERS_PARALLELISM"] = "false"

class FixedT5Trainer:
    def __init__(self):
        self.setup_logging()
        self.load_config()
        self.setup_device()
        self.setup_wandb()
        self.log_initial_info()
        self.load_model()

    def setup_logging(self):
        """è¨­ç½®ç°¡åŒ–æ—¥èªŒ"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S',
            force=True
        )
        self.logger = logging.getLogger(__name__)
        print("=" * 60)
        print("ğŸš€ ä¿®å¾©ç‰ˆ GPU è¨“ç·´é–‹å§‹")
        print("=" * 60)

    def load_config(self):
        """è¼‰å…¥é…ç½®"""
        self.model_name = os.getenv('MODEL_NAME', 'Salesforce/codet5-base')
        self.max_input_length = int(os.getenv('MAX_INPUT_LENGTH', '512'))
        self.max_output_length = int(os.getenv('MAX_OUTPUT_LENGTH', '128'))

        # æ‰¹æ¬¡å¤§å°è¨­ç½®
        self.batch_size = int(os.getenv('BATCH_SIZE', '4'))
        self.gradient_accumulation_steps = int(os.getenv('GRAD_ACCUM', '8'))

        # å­¸ç¿’ç‡è¨­ç½®
        self.learning_rate = float(os.getenv('LEARNING_RATE', '1e-4'))
        self.epochs = int(os.getenv('EPOCHS', '8'))
        self.warmup_ratio = float(os.getenv('WARMUP_RATIO', '0.1'))

        self.output_dir = os.getenv('OUTPUT_DIR', './commit-model')

        self.config_info = {
            "model": self.model_name,
            "max_input_length": self.max_input_length,
            "max_output_length": self.max_output_length,
            "batch_size": self.batch_size,
            "gradient_accumulation_steps": self.gradient_accumulation_steps,
            "effective_batch_size": self.batch_size * self.gradient_accumulation_steps,
            "learning_rate": self.learning_rate,
            "epochs": self.epochs,
            "warmup_ratio": self.warmup_ratio,
            "output_dir": self.output_dir
        }

        print("ğŸ“‹ ä¿®å¾©ç‰ˆé…ç½®åƒæ•¸:")
        print(f"  æ¨¡å‹: {self.model_name}")
        print(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {self.batch_size * self.gradient_accumulation_steps}")
        print(f"  å­¸ç¿’ç‡: {self.learning_rate}")
        print(f"  è¨“ç·´è¼ªæ•¸: {self.epochs}")

    def setup_device(self):
        """è¨­ç½®è¨­å‚™ä¸¦è™•ç† CUDA åˆå§‹åŒ–"""
        # æ›´å®‰å…¨çš„ CUDA åˆå§‹åŒ–
        if torch.cuda.is_available():
            try:
                self.device = torch.device("cuda")
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3

                self.device_info = {
                    "device": str(self.device),
                    "gpu_name": gpu_name,
                    "gpu_memory_gb": round(gpu_memory, 1)
                }

                print(f"ğŸš€ GPU: {gpu_name} ({gpu_memory:.1f} GB)")

                # æ›´ä¿å®ˆçš„è¨˜æ†¶é«”ç®¡ç†
                torch.cuda.empty_cache()
                torch.cuda.set_per_process_memory_fraction(0.8)

            except Exception as e:
                print(f"âš ï¸ CUDA åˆå§‹åŒ–å•é¡Œ: {e}")
                self.device = torch.device("cpu")
                self.device_info = {"device": "cpu", "cuda_error": str(e)}
        else:
            self.device = torch.device("cpu")
            self.device_info = {"device": "cpu"}
            print("âš ï¸ ä½¿ç”¨ CPU")

    def setup_wandb(self):
        """åˆå§‹åŒ– WandB"""
        try:
            wandb.init(
                project="commit-message-generator-fixed",
                name=f"fixed-t5-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                config=self.config_info
            )
            print("ğŸ“Š WandB åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ WandB åˆå§‹åŒ–å¤±æ•—: {e}")

    def log_initial_info(self):
        """è¨˜éŒ„åˆå§‹åŒ–è³‡è¨Š"""
        try:
            wandb.log({"status": "initialization_complete"})
            wandb.log({"config": self.config_info})
            wandb.log({"device_info": self.device_info})
        except:
            pass  # WandB è¨˜éŒ„å¤±æ•—ä¸å½±éŸ¿è¨“ç·´

    def load_model(self):
        """è¼‰å…¥æ¨¡å‹"""
        print(f"ğŸ”„ è¼‰å…¥æ¨¡å‹: {self.model_name}")
        start_time = time.time()

        try:
            self.tokenizer = RobertaTokenizer.from_pretrained(self.model_name)

            # æ›´å®‰å…¨çš„æ¨¡å‹è¼‰å…¥
            if self.device.type == 'cuda':
                try:
                    self.model = T5ForConditionalGeneration.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float32
                    ).to(self.device)
                except Exception as e:
                    print(f"âš ï¸ GPU è¼‰å…¥å¤±æ•—ï¼Œæ”¹ç”¨ CPU: {e}")
                    self.device = torch.device("cpu")
                    self.model = T5ForConditionalGeneration.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float32
                    )
            else:
                self.model = T5ForConditionalGeneration.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float32
                )

            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            load_time = time.time() - start_time
            print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ ({load_time:.2f}s)")

        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            raise

    def get_training_data_paths(self, data_directory="training-data"):
        """ç²å–è¨“ç·´æ•¸æ“šè·¯å¾‘"""
        pattern = os.path.join(data_directory, "*.json")
        data_paths = glob.glob(pattern)

        if not data_paths:
            print(f"âš ï¸ åœ¨ {data_directory} ç›®éŒ„ä¸­æœªæ‰¾åˆ° JSON æ–‡ä»¶")
            return []

        data_paths.sort()
        print(f"ğŸ“‚ ç™¼ç¾ {len(data_paths)} å€‹è¨“ç·´æ–‡ä»¶")
        return data_paths

    def analyze_and_filter_data(self, data, data_type="è³‡æ–™"):
        """åˆ†æä¸¦éæ¿¾æ•¸æ“šè³ªé‡"""
        print(f"ğŸ” åˆ†æ{data_type}è³ªé‡...")

        filtered_data = []
        stats = {
            "total": len(data),
            "too_short_input": 0,
            "too_long_input": 0,
            "too_short_output": 0,
            "too_long_output": 0,
            "valid": 0
        }

        for item in data:
            try:
                if not isinstance(item, dict) or 'input' not in item or 'output' not in item:
                    continue

                input_text = str(item['input']).strip()
                output_text = str(item['output']).strip()

                input_len = len(input_text.split())
                output_len = len(output_text.split())

                # éæ¿¾æ¢ä»¶
                if input_len < 10:
                    stats["too_short_input"] += 1
                    continue
                if input_len > 200:
                    stats["too_long_input"] += 1
                    continue
                if output_len < 3:
                    stats["too_short_output"] += 1
                    continue
                if output_len > 50:
                    stats["too_long_output"] += 1
                    continue

                filtered_data.append({
                    'input_text': input_text,
                    'target_text': output_text
                })
                stats["valid"] += 1

            except Exception as e:
                continue

        print(f"ğŸ“Š {data_type}éæ¿¾çµ±è¨ˆ:")
        print(f"  ç¸½æ•¸: {stats['total']}")
        print(f"  æœ‰æ•ˆ: {stats['valid']}")

        try:
            wandb.log({f"{data_type}_filter_stats": stats})
        except:
            pass

        return filtered_data

    def load_data(self, data_paths):
        """è¼‰å…¥è³‡æ–™"""
        print("ğŸ“‚ è¼‰å…¥è³‡æ–™...")
        all_data = []

        for path in data_paths:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                all_data.extend(data)
                print(f"âœ… {os.path.basename(path)}: {len(data):,} ç­†")
            except Exception as e:
                print(f"âŒ è¼‰å…¥å¤±æ•— {path}: {e}")

        random.seed(42)
        random.shuffle(all_data)

        total = len(all_data)
        train_size = int(total * 0.85)
        train_data = all_data[:train_size]
        val_data = all_data[train_size:]

        print(f"ğŸ“Š æ•¸æ“šåˆ†å‰²: è¨“ç·´ {len(train_data):,} / é©—è­‰ {len(val_data):,}")
        return train_data, val_data

    def preprocess_data(self, data, data_type="è³‡æ–™"):
        """ä¿®å¾©ç‰ˆè³‡æ–™é è™•ç†"""
        print(f"ğŸ”„ é è™•ç†{data_type}...")

        processed_data = self.analyze_and_filter_data(data, data_type)

        if not processed_data:
            raise ValueError(f"æ²’æœ‰æœ‰æ•ˆçš„{data_type}")

        def tokenize_function(examples):
            inputs = examples['input_text']
            targets = examples['target_text']

            model_inputs = self.tokenizer(
                inputs,
                max_length=self.max_input_length,
                padding=False,
                truncation=True,
                return_tensors=None
            )

            with self.tokenizer.as_target_tokenizer():
                labels = self.tokenizer(
                    targets,
                    max_length=self.max_output_length,
                    padding=False,
                    truncation=True,
                    return_tensors=None
                )

            model_inputs["labels"] = labels["input_ids"]
            return model_inputs

        dataset = Dataset.from_list(processed_data)

        # ç¦ç”¨å¤šé€²ç¨‹è™•ç†é¿å… CUDA éŒ¯èª¤
        print(f"ğŸ”§ ä½¿ç”¨å–®é€²ç¨‹ tokenization (ä¿®å¾© CUDA multiprocessing å•é¡Œ)")
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
            desc=f"Tokenizing {data_type}",
            num_proc=None  # ç¦ç”¨å¤šé€²ç¨‹
        )

        return tokenized_dataset

    def train(self, data_directory="training-data"):
        """ä¿®å¾©ç‰ˆè¨“ç·´æµç¨‹"""
        print("ğŸš€ é–‹å§‹ä¿®å¾©ç‰ˆè¨“ç·´...")
        training_start_time = time.time()

        try:
            data_paths = self.get_training_data_paths(data_directory)
            if not data_paths:
                raise ValueError("æœªæ‰¾åˆ°è¨“ç·´æ•¸æ“š")

            train_data, val_data = self.load_data(data_paths)
            train_dataset = self.preprocess_data(train_data, "è¨“ç·´è³‡æ–™")
            val_dataset = self.preprocess_data(val_data, "é©—è­‰è³‡æ–™") if val_data else None

            data_collator = DataCollatorForSeq2Seq(
                tokenizer=self.tokenizer,
                model=self.model,
                padding=True,
                return_tensors="pt"
            )

            total_steps = (len(train_dataset) // (self.batch_size * self.gradient_accumulation_steps)) * self.epochs
            warmup_steps = int(total_steps * self.warmup_ratio)

            print(f"ğŸ“ˆ è¨“ç·´æ­¥æ•¸: {total_steps}")
            print(f"ğŸ”¥ Warmup æ­¥æ•¸: {warmup_steps}")

            # ä¿®å¾©ç‰ˆè¨“ç·´åƒæ•¸
            training_args = TrainingArguments(
                output_dir=self.output_dir,
                num_train_epochs=self.epochs,

                # æ‰¹æ¬¡è¨­ç½®
                per_device_train_batch_size=self.batch_size,
                per_device_eval_batch_size=self.batch_size,
                gradient_accumulation_steps=self.gradient_accumulation_steps,

                # å­¸ç¿’ç‡è¨­ç½®
                learning_rate=self.learning_rate,
                lr_scheduler_type="cosine",
                warmup_steps=warmup_steps,
                weight_decay=0.01,

                # æ›´ä¿å®ˆçš„ä¸¦è¡Œè¨­ç½®
                fp16=False,  # æš«æ™‚ç¦ç”¨æ··åˆç²¾åº¦é¿å…å•é¡Œ
                bf16=False,

                # ç¦ç”¨æ‰€æœ‰å¤šé€²ç¨‹åŠŸèƒ½
                max_grad_norm=1.0,
                dataloader_num_workers=0,  # ç¦ç”¨å¤šé€²ç¨‹
                dataloader_pin_memory=False,  # ç¦ç”¨ pin memory

                # è©•ä¼°å’Œä¿å­˜
                eval_strategy="steps" if val_dataset else "no",
                eval_steps=100 if val_dataset else None,
                save_steps=200,
                logging_steps=25,

                # æ¨¡å‹ä¿å­˜
                save_total_limit=3,
                load_best_model_at_end=True if val_dataset else False,
                metric_for_best_model="eval_loss" if val_dataset else None,
                greater_is_better=False,

                # å…¶ä»–è¨­ç½®
                remove_unused_columns=True,
                prediction_loss_only=True,
                seed=42,
                report_to="wandb" if wandb.run else None,
                logging_dir=f"{self.output_dir}/logs",
                run_name=f"fixed-training-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )

            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=train_dataset,
                eval_dataset=val_dataset,
                data_collator=data_collator,
                tokenizer=self.tokenizer,
            )

            print("ğŸ¯ é–‹å§‹è¨“ç·´...")
            train_result = trainer.train()
            training_time = time.time() - training_start_time

            print(f"ğŸ‰ è¨“ç·´å®Œæˆ! è€—æ™‚: {training_time:.2f} ç§’")

            # ä¿å­˜æ¨¡å‹
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {self.output_dir}")

            # è¨˜éŒ„çµæœ
            try:
                final_results = {
                    "training_time": training_time,
                    "final_loss": getattr(train_result, 'training_loss', None),
                    "total_steps": total_steps,
                    "model_saved": True
                }
                wandb.log({"final_results": final_results})
            except:
                pass

        except Exception as e:
            print(f"âŒ è¨“ç·´éŒ¯èª¤: {e}")
            try:
                wandb.log({"training_error": str(e)})
            except:
                pass
            raise
        finally:
            try:
                wandb.finish()
            except:
                pass

def main():
    """ä¸»å‡½æ•¸"""
    try:
        trainer = FixedT5Trainer()
        trainer.train(data_directory="training-data")
        print("âœ… ä¿®å¾©ç‰ˆè¨“ç·´å®Œæˆ!")

    except Exception as e:
        print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()