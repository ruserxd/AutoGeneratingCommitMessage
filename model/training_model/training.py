import torch
from datasets import Dataset
from transformers import (
  T5ForConditionalGeneration,
  RobertaTokenizer,
  TrainingArguments,
  Trainer,
  EarlyStoppingCallback
)
import numpy as np
import json
import random
import os


# è¨­ç½®GPUè¨˜æ†¶é«”é™åˆ¶
def setup_gpu_limits():
  """è¨­ç½®GPUä½¿ç”¨é™åˆ¶"""

  # æª¢æŸ¥æ˜¯å¦æœ‰MPSæ”¯æ´ï¼ˆM1/M2/M3/M4 Macï¼‰
  if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("ğŸ¯ ä½¿ç”¨ MPS (Apple Silicon GPU)")

    # è¨­ç½®MPSè¨˜æ†¶é«”åˆ†é…ç­–ç•¥
    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.7'  # ä½¿ç”¨70%çš„GPUè¨˜æ†¶é«”

    # å•Ÿç”¨è¨˜æ†¶é«”æ•ˆç‡æ¨¡å¼
    torch.mps.empty_cache()  # æ¸…ç©ºå¿«å–

  elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("ğŸ¯ ä½¿ç”¨ CUDA GPU")

    # è¨­ç½®CUDAè¨˜æ†¶é«”é™åˆ¶ï¼ˆä»¥GBç‚ºå–®ä½ï¼‰
    torch.cuda.set_per_process_memory_fraction(0.7)  # ä½¿ç”¨70%çš„GPUè¨˜æ†¶é«”
    torch.cuda.empty_cache()

  else:
    device = torch.device("cpu")
    print("ğŸ’» ä½¿ç”¨ CPU")

  return device


model_path = "./commit-model"


class ValidationTrainer:
  def __init__(self, model_name='Salesforce/codet5-base'):
    self.model_name = model_name
    self.device = setup_gpu_limits()  # è¨­ç½®GPUé™åˆ¶

    self.tokenizer = RobertaTokenizer.from_pretrained(model_name)
    self.model = T5ForConditionalGeneration.from_pretrained(model_name)

    # å°‡æ¨¡å‹ç§»åˆ°æŒ‡å®šè¨­å‚™
    self.model = self.model.to(self.device)

    # è¨­ç½®pad_token
    if self.tokenizer.pad_token is None:
      self.tokenizer.pad_token = self.tokenizer.eos_token

  # è¨“ç·´é›† 70% é©—è­‰é›† 20% æ¸¬è©¦é›† 10%
  def load_and_split_data(self, data_path, test_size=0.2, val_size=0.1):
    """è¼‰å…¥æ•¸æ“šä¸¦åˆ†å‰²ç‚ºè¨“ç·´/é©—è­‰/æ¸¬è©¦é›†"""

    with open(data_path, 'r', encoding='utf-8') as f:
      data = json.load(f)

    # å¯¦ç¾æ•¸æ“šåˆ†å‰²
    random.seed(42)  # å›ºå®šéš¨æ©Ÿç¨®å­
    data_shuffled = data.copy()
    random.shuffle(data_shuffled)

    total_size = len(data_shuffled)
    test_count = int(total_size * test_size)
    val_count = int(total_size * val_size)

    # åˆ†å‰²æ•¸æ“š
    test_data = data_shuffled[:test_count]
    val_data = data_shuffled[test_count:test_count + val_count]
    train_data = data_shuffled[test_count + val_count:]

    print(f"ğŸ“Š æ•¸æ“šåˆ†å‰²çµæœ:")
    print(f"  è¨“ç·´é›†: {len(train_data)} ç­†")
    print(f"  é©—è­‰é›†: {len(val_data)} ç­†")
    print(f"  æ¸¬è©¦é›†: {len(test_data)} ç­†")

    return train_data, val_data, test_data

  def preprocess_data(self, data, max_input_length=512, max_target_length=128):
    """é è™•ç†æ•¸æ“š"""

    def tokenize_function(examples):
      inputs = examples['input']
      targets = examples['output']

      # Tokenizeè¼¸å…¥
      model_inputs = self.tokenizer(
          inputs,
          max_length=max_input_length,
          padding='max_length',
          truncation=True,
          return_tensors=None
      )

      # Tokenizeæ¨™ç±¤
      with self.tokenizer.as_target_tokenizer():
        labels = self.tokenizer(
            targets,
            max_length=max_target_length,
            padding='max_length',
            truncation=True,
            return_tensors=None
        )

      model_inputs["labels"] = labels["input_ids"]
      return model_inputs

    # è½‰æ›ç‚ºDatasetæ ¼å¼
    dataset = Dataset.from_list(data)

    # æ‡‰ç”¨tokenization
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=dataset.column_names
    )

    return tokenized_dataset

  def compute_metrics(self, eval_pred):
    """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
    predictions, labels = eval_pred

    if isinstance(predictions, tuple):
      predictions = predictions[0]  # å–ç¬¬ä¸€å€‹å…ƒç´ ï¼ˆlogitsï¼‰

    # ç¢ºä¿ predictions æ˜¯ numpy array
    if hasattr(predictions, 'cpu'):
      predictions = predictions.cpu().numpy()
    elif not isinstance(predictions, np.ndarray):
      predictions = np.array(predictions)

    # è™•ç†é æ¸¬çµæœçš„argmax
    if predictions.ndim == 3:
      predictions = np.argmax(predictions, axis=-1)

    # è§£ç¢¼é æ¸¬çµæœ
    decoded_preds = self.tokenizer.batch_decode(
        predictions,
        skip_special_tokens=True
    )

    # è™•ç†æ¨™ç±¤ï¼ˆæ›¿æ›-100ç‚ºpad_token_idï¼‰
    labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)
    decoded_labels = self.tokenizer.batch_decode(
        labels,
        skip_special_tokens=True
    )

    # è¨ˆç®—BLEUåˆ†æ•¸
    try:
      from nltk.translate.bleu_score import sentence_bleu

      bleu_scores = []
      for pred, label in zip(decoded_preds, decoded_labels):
        pred_tokens = pred.split()
        label_tokens = label.split()

        if len(label_tokens) == 0:
          score = 0.0
        else:
          score = sentence_bleu(
              [label_tokens],
              pred_tokens,
              weights=(1, 0, 0, 0)  # åªè¨ˆç®—1-gram
          )
        bleu_scores.append(score)

      return {
        'bleu': np.mean(bleu_scores),
        'avg_length': np.mean([len(pred.split()) for pred in decoded_preds])
      }
    except ImportError:
      overlap_scores = []
      for pred, label in zip(decoded_preds, decoded_labels):
        pred_words = set(pred.split())
        label_words = set(label.split())
        if len(label_words) > 0:
          overlap = len(pred_words & label_words) / len(label_words)
        else:
          overlap = 0
        overlap_scores.append(overlap)

      return {
        'word_overlap': np.mean(overlap_scores),
        'avg_length': np.mean([len(pred.split()) for pred in decoded_preds])
      }

  def train_with_validation(self, train_data, val_data, output_dir=model_path):
    """ä½¿ç”¨é©—è­‰é›†é€²è¡Œè¨“ç·´"""

    # é è™•ç†æ•¸æ“š
    train_dataset = self.preprocess_data(train_data)
    val_dataset = self.preprocess_data(val_data)

    # è¨­å®šè¨“ç·´åƒæ•¸ - é‡å°M4 Proå„ªåŒ–
    training_args = TrainingArguments(
        output_dir=output_dir,

        # åŸºæœ¬åƒæ•¸ - é™ä½batch sizeä»¥ç¯€çœè¨˜æ†¶é«”
        num_train_epochs=10,
        per_device_train_batch_size=1,  # å¾2é™åˆ°1
        per_device_eval_batch_size=1,  # å¾2é™åˆ°1
        gradient_accumulation_steps=4,  # å¾2å¢åŠ åˆ°4ä¾†è£œå„Ÿbatch sizeæ¸›å°‘

        # å­¸ç¿’ç‡å’Œå„ªåŒ–å™¨
        learning_rate=5e-5,
        warmup_steps=100,
        weight_decay=0.01,

        # è©•ä¼°å’Œä¿å­˜
        eval_strategy="steps",
        eval_steps=100,  # å¢åŠ è©•ä¼°é–“éš”
        save_strategy="steps",
        save_steps=100,  # å¢åŠ ä¿å­˜é–“éš”
        save_total_limit=2,  # æ¸›å°‘ä¿å­˜çš„checkpointæ•¸é‡

        # ç›£æ§å’Œæ—¥èªŒ
        logging_dir=f'{output_dir}/logs',
        logging_steps=25,
        report_to="none",

        # Early stopping
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,

        # è¨˜æ†¶é«”å„ªåŒ–è¨­ç½®
        dataloader_pin_memory=False,
        remove_unused_columns=True,  # ç§»é™¤ä¸ä½¿ç”¨çš„æ¬„ä½
        fp16=False,  # M4 Proä½¿ç”¨MPSæ™‚å»ºè­°é—œé–‰fp16
        dataloader_num_workers=0,  # è¨­ç‚º0é¿å…å¤šé€²ç¨‹å•é¡Œ

        # æ¢¯åº¦æª¢æŸ¥é»ä»¥ç¯€çœè¨˜æ†¶é«”
        gradient_checkpointing=True,
    )

    # å‰µå»ºTrainer
    trainer = Trainer(
        model=self.model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=self.tokenizer,
        compute_metrics=self.compute_metrics,
        callbacks=[
          EarlyStoppingCallback(
              early_stopping_patience=5,  # å¢åŠ patience
              early_stopping_threshold=0.01
          )
        ]
    )

    # é–‹å§‹è¨“ç·´
    print("ğŸš€ é–‹å§‹è¨“ç·´ï¼ˆå«é©—è­‰ï¼‰...")
    print(f"ğŸ’¾ ä½¿ç”¨è¨­å‚™: {self.device}")

    # å®šæœŸæ¸…ç†è¨˜æ†¶é«”
    def cleanup_memory():
      if self.device.type == "mps":
        torch.mps.empty_cache()
      elif self.device.type == "cuda":
        torch.cuda.empty_cache()

    trainer.train()
    cleanup_memory()  # è¨“ç·´å®Œæˆå¾Œæ¸…ç†è¨˜æ†¶é«”

    # ä¿å­˜æœ€çµ‚æ¨¡å‹
    trainer.save_model()
    self.tokenizer.save_pretrained(output_dir)

    # è¿”å›è¨“ç·´æ­·å²
    return trainer.state.log_history

  def evaluate_model(self, test_data, model_path=model_path):
    """åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°æ¨¡å‹"""

    # è¼‰å…¥è¨“ç·´å¥½çš„æ¨¡å‹
    model = T5ForConditionalGeneration.from_pretrained(model_path)
    model = model.to(self.device)
    model.eval()

    test_dataset = self.preprocess_data(test_data)

    # å‰µå»ºè©•ä¼°trainer
    trainer = Trainer(
        model=model,
        tokenizer=self.tokenizer,
        compute_metrics=self.compute_metrics
    )

    # è©•ä¼°
    print("ğŸ“Š åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°...")
    results = trainer.evaluate(test_dataset)

    print(f"æ¸¬è©¦é›†çµæœ:")
    for key, value in results.items():
      print(f"  {key}: {value:.4f}")

    return results

  def plot_training_curves(self, log_history):
    """ç¹ªè£½è¨“ç·´æ›²ç·š"""
    import matplotlib.pyplot as plt

    # æå–è¨“ç·´å’Œé©—è­‰loss
    train_losses = []
    val_losses = []
    train_steps = []
    val_steps = []

    for log in log_history:
      if 'loss' in log and 'step' in log:
        train_losses.append(log['loss'])
        train_steps.append(log['step'])
      if 'eval_loss' in log and 'step' in log:
        val_losses.append(log['eval_loss'])
        val_steps.append(log['step'])

    # ç¹ªè£½åœ–è¡¨
    plt.figure(figsize=(12, 4))

    # è¨“ç·´loss
    plt.subplot(1, 2, 1)
    if train_losses:
      plt.plot(train_steps, train_losses, label='Training Loss')
      plt.xlabel('Steps')
      plt.ylabel('Loss')
      plt.title('Training Loss')
      plt.legend()

    # é©—è­‰loss
    plt.subplot(1, 2, 2)
    if val_losses:
      plt.plot(val_steps, val_losses, label='Validation Loss', color='orange')
      plt.xlabel('Steps')
      plt.ylabel('Loss')
      plt.title('Validation Loss')
      plt.legend()

    plt.tight_layout()
    plt.savefig('./training_curves.png')
    plt.show()

  def generate_commit_message(self, diff_text, model_path=model_path):
    """ç”Ÿæˆcommit message"""

    # è¼‰å…¥æ¨¡å‹
    model = T5ForConditionalGeneration.from_pretrained(model_path)
    model = model.to(self.device)
    model.eval()

    # é è™•ç†è¼¸å…¥
    inputs = self.tokenizer(
        diff_text,
        max_length=512,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    # å°‡è¼¸å…¥ç§»åˆ°æ­£ç¢ºçš„è¨­å‚™
    inputs = {k: v.to(self.device) for k, v in inputs.items()}

    # ç”Ÿæˆè¼¸å‡º
    with torch.no_grad():
      outputs = model.generate(
          inputs['input_ids'],
          attention_mask=inputs['attention_mask'],
          max_length=128,
          num_beams=4,
          temperature=0.7,
          do_sample=True,
          pad_token_id=self.tokenizer.pad_token_id
      )

    # è§£ç¢¼çµæœ
    commit_message = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
    return commit_message


def monitor_memory_usage():
  """ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
  if torch.backends.mps.is_available():
    # M4 Proè¨˜æ†¶é«”ç›£æ§
    import psutil
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"ğŸ’¾ RAMä½¿ç”¨é‡: {memory_info.rss / 1024 / 1024 / 1024:.2f} GB")

    # MPSæ²’æœ‰ç›´æ¥çš„è¨˜æ†¶é«”æŸ¥è©¢APIï¼Œä½†å¯ä»¥é€šéç³»çµ±ç›£æ§
    torch.mps.empty_cache()

  elif torch.cuda.is_available():
    allocated = torch.cuda.memory_allocated() / 1024 ** 3
    cached = torch.cuda.memory_reserved() / 1024 ** 3
    print(f"ğŸ’¾ GPUè¨˜æ†¶é«” - å·²åˆ†é…: {allocated:.2f} GB, å¿«å–: {cached:.2f} GB")


def main():
  # å‰µå»ºè¨“ç·´å™¨
  trainer = ValidationTrainer()

  # ç›£æ§åˆå§‹è¨˜æ†¶é«”ä½¿ç”¨
  print("ğŸ” åˆå§‹è¨˜æ†¶é«”ç‹€æ…‹:")
  monitor_memory_usage()

  # è¼‰å…¥å’Œåˆ†å‰²æ•¸æ“š
  train_data, val_data, test_data = trainer.load_and_split_data(
      "spider-data/training-data/spring-boot/spring-boot-training.json"
  )

  # è¨“ç·´æ¨¡å‹
  print("\nğŸ” è¨“ç·´å‰è¨˜æ†¶é«”ç‹€æ…‹:")
  monitor_memory_usage()

  log_history = trainer.train_with_validation(train_data, val_data)

  print("\nğŸ” è¨“ç·´å¾Œè¨˜æ†¶é«”ç‹€æ…‹:")
  monitor_memory_usage()

  # ç¹ªè£½è¨“ç·´æ›²ç·š
  trainer.plot_training_curves(log_history)

  # åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°
  trainer.evaluate_model(test_data)

  print("\nğŸ” æœ€çµ‚è¨˜æ†¶é«”ç‹€æ…‹:")
  monitor_memory_usage()


if __name__ == "__main__":
  main()