import json
import re
import argparse
import glob
import os
from collections import Counter, defaultdict
from typing import Dict, List, Any, Tuple
from pathlib import Path


def extract_first_word(text: str) -> str:
  """
  æå–æ–‡æœ¬çš„ç¬¬ä¸€å€‹å–®è©

  Args:
      text: è¼¸å…¥æ–‡æœ¬

  Returns:
      ç¬¬ä¸€å€‹å–®è©ï¼ˆè½‰ç‚ºå°å¯«ï¼‰
  """
  if not text:
    return ""

  # ç§»é™¤é–‹é ­çš„ç©ºç™½å­—ç¬¦
  text = text.strip()

  # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–ç¬¬ä¸€å€‹å–®è©
  # æ”¯æ´è‹±æ–‡å–®è©ã€ä¸­æ–‡å­—ç¬¦ã€æ•¸å­—ç­‰
  match = re.match(r'[\w\u4e00-\u9fff]+', text)

  if match:
    return match.group().lower()
  else:
    # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›ç¬¬ä¸€å€‹éç©ºç™½å­—ç¬¦
    for char in text:
      if not char.isspace():
        return char.lower()
    return ""


def analyze_single_file(file_path: str) -> Dict[str, Any]:
  """
  åˆ†æå–®å€‹æª”æ¡ˆï¼Œè¿”å›çµ±è¨ˆçµæœ

  Args:
      file_path: JSON æª”æ¡ˆè·¯å¾‘

  Returns:
      åŒ…å«çµ±è¨ˆè³‡æ–™çš„å­—å…¸
  """
  try:
    with open(file_path, 'r', encoding='utf-8') as f:
      data = json.load(f)

    # æ”¶é›†ç¬¬ä¸€å€‹å­—å’Œå°æ‡‰çš„å®Œæ•´æ–‡æœ¬ç¯„ä¾‹
    first_words = []
    word_examples = defaultdict(list)

    for i, item in enumerate(data):
      if 'output' in item and item['output']:
        output_text = item['output']
        first_word = extract_first_word(output_text)

        if first_word:
          first_words.append(first_word)

          # å„²å­˜ç¯„ä¾‹ï¼ˆæ¯å€‹å­—æœ€å¤šä¿å­˜3å€‹ç¯„ä¾‹ï¼‰
          if len(word_examples[first_word]) < 3:
            word_examples[first_word].append({
              'index': i + 1,
              'text': output_text
            })

    # çµ±è¨ˆé »ç‡
    word_counter = Counter(first_words)

    return {
      'file_path': file_path,
      'total_entries': len(data),
      'valid_entries': len(first_words),
      'word_counter': word_counter,
      'word_examples': dict(word_examples),
      'success': True,
      'error': None
    }

  except Exception as e:
    return {
      'file_path': file_path,
      'success': False,
      'error': str(e),
      'word_counter': Counter(),
      'word_examples': {}
    }


def analyze_multiple_files(file_patterns: List[str], show_examples: bool = True,
    min_count: int = 1, show_per_file: bool = True) -> None:
  """
  åˆ†æå¤šå€‹æª”æ¡ˆä¸­ output æ¬„ä½çš„ç¬¬ä¸€å€‹å­—

  Args:
      file_patterns: æª”æ¡ˆè·¯å¾‘æˆ–æ¨¡å¼åˆ—è¡¨
      show_examples: æ˜¯å¦é¡¯ç¤ºç¯„ä¾‹
      min_count: æœ€å°å‡ºç¾æ¬¡æ•¸éæ¿¾
      show_per_file: æ˜¯å¦é¡¯ç¤ºæ¯å€‹æª”æ¡ˆçš„åˆ†æ
  """
  # æ”¶é›†æ‰€æœ‰æª”æ¡ˆ
  all_files = []
  for pattern in file_patterns:
    if os.path.isfile(pattern):
      all_files.append(pattern)
    else:
      # æ”¯æ´è¬ç”¨å­—ç¬¦
      matched_files = glob.glob(pattern)
      all_files.extend(matched_files)

  if not all_files:
    print("âŒ æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æª”æ¡ˆ")
    return

  # å»é‡ä¸¦æ’åº
  all_files = sorted(list(set(all_files)))

  print(f"ğŸ” æ‰¾åˆ° {len(all_files)} å€‹æª”æ¡ˆè¦åˆ†æ")
  print("=" * 80)

  # åˆ†ææ¯å€‹æª”æ¡ˆ
  file_results = []
  combined_counter = Counter()
  combined_examples = defaultdict(list)

  for file_path in all_files:
    print(f"ğŸ“‚ æ­£åœ¨åˆ†æ: {file_path}")
    result = analyze_single_file(file_path)
    file_results.append(result)

    if result['success']:
      combined_counter.update(result['word_counter'])

      # åˆä½µç¯„ä¾‹
      for word, examples in result['word_examples'].items():
        for example in examples:
          if len(combined_examples[word]) < 5:  # æ¯å€‹å­—æœ€å¤šä¿å­˜5å€‹ç¯„ä¾‹
            example['file'] = os.path.basename(file_path)
            combined_examples[word].append(example)

      print(
        f"  âœ… æˆåŠŸ: {result['valid_entries']}/{result['total_entries']} ç­†è³‡æ–™")
    else:
      print(f"  âŒ å¤±æ•—: {result['error']}")

  print("\n" + "=" * 80)

  # é¡¯ç¤ºå€‹åˆ¥æª”æ¡ˆçµ±è¨ˆ
  if show_per_file:
    print("ğŸ“Š å€‹åˆ¥æª”æ¡ˆçµ±è¨ˆ:")
    print("-" * 80)

    for result in file_results:
      if result['success']:
        file_name = os.path.basename(result['file_path'])
        word_count = len(result['word_counter'])
        top_word = result['word_counter'].most_common(1)
        top_word_info = f"{top_word[0][0]} ({top_word[0][1]}æ¬¡)" if top_word else "ç„¡"

        print(f"ğŸ“„ {file_name:<30} | "
              f"è³‡æ–™: {result['valid_entries']:>5} | "
              f"ä¸åŒå­—: {word_count:>3} | "
              f"æœ€å¸¸è¦‹: {top_word_info}")

    print("\n" + "=" * 80)

  # é¡¯ç¤ºåˆä½µçµ±è¨ˆ
  total_valid = sum(r['valid_entries'] for r in file_results if r['success'])
  total_entries = sum(r['total_entries'] for r in file_results if r['success'])

  print("ğŸŒ åˆä½µçµ±è¨ˆçµæœ:")
  print(
    f"ğŸ“ åˆ†ææª”æ¡ˆæ•¸: {len([r for r in file_results if r['success']])}/{len(file_results)}")
  print(f"ğŸ“Š ç¸½è³‡æ–™ç­†æ•¸: {total_entries}")
  print(f"ğŸ“ˆ æœ‰æ•ˆè³‡æ–™æ•¸: {total_valid}")
  print(f"ğŸ”¤ ä¸åŒç¬¬ä¸€å­—: {len(combined_counter)}")

  # éæ¿¾æœ€å°å‡ºç¾æ¬¡æ•¸
  filtered_words = {word: count for word, count in combined_counter.items()
                    if count >= min_count}

  if not filtered_words:
    print(f"âŒ æ²’æœ‰ç¬¦åˆæœ€å°å‡ºç¾æ¬¡æ•¸ ({min_count}) çš„å­—")
    return

  print(f"ğŸ” ç¬¦åˆæ¢ä»¶çš„å­—: {len(filtered_words)} å€‹ (å‡ºç¾æ¬¡æ•¸ >= {min_count})")
  print()

  # æŒ‰é »ç‡æ’åºé¡¯ç¤º
  sorted_words = sorted(filtered_words.items(), key=lambda x: (-x[1], x[0]))

  print("ğŸ“‹ ç¬¬ä¸€å€‹å­—çµ±è¨ˆ (åˆä½µæ‰€æœ‰æª”æ¡ˆï¼ŒæŒ‰é »ç‡æ’åº):")
  print("-" * 80)
  print(f"{'ç¬¬ä¸€å€‹å­—':<15} {'æ¬¡æ•¸':<8} {'ç™¾åˆ†æ¯”':<8} {'æª”æ¡ˆæ•¸':<8} {'ç¯„ä¾‹'}")
  print("-" * 80)

  total_count = sum(filtered_words.values())

  for word, count in sorted_words[:50]:  # é¡¯ç¤ºå‰50å€‹
    percentage = (count / total_count) * 100

    # è¨ˆç®—å‡ºç¾åœ¨å¹¾å€‹æª”æ¡ˆä¸­
    files_with_word = set()
    for result in file_results:
      if result['success'] and word in result['word_counter']:
        files_with_word.add(result['file_path'])
    file_count = len(files_with_word)

    # é¡¯ç¤ºç¬¬ä¸€å€‹ç¯„ä¾‹
    example = ""
    if word in combined_examples and combined_examples[word]:
      example_text = combined_examples[word][0]['text']
      example = example_text[:35] + "..." if len(
        example_text) > 35 else example_text
      example = example.replace('\n', ' ')

    print(
      f"{word:<15} {count:<8} {percentage:<7.1f}% {file_count:<8} {example}")

  if len(sorted_words) > 50:
    print(f"\n... é‚„æœ‰ {len(sorted_words) - 50} å€‹ç¬¬ä¸€å€‹å­—")

  # é¡¯ç¤ºè©³ç´°ç¯„ä¾‹
  if show_examples:
    print("\n" + "=" * 80)
    print("ğŸ“ è©³ç´°ç¯„ä¾‹ (å‰10å€‹æœ€å¸¸è¦‹çš„ç¬¬ä¸€å€‹å­—):")
    print("=" * 80)

    for word, count in sorted_words[:10]:
      print(f"\nğŸ”¤ '{word}' (å‡ºç¾ {count} æ¬¡):")

      if word in combined_examples:
        for j, example in enumerate(combined_examples[word], 1):
          file_name = example.get('file', 'unknown')
          print(f"  ç¯„ä¾‹ {j} ({file_name}): {example['text']}")
      print("-" * 60)


def export_multiple_results(file_patterns: List[str],
    output_file: str = None) -> None:
  """
  å°‡å¤šæª”æ¡ˆç¬¬ä¸€å€‹å­—çµ±è¨ˆçµæœåŒ¯å‡ºåˆ° JSON æª”æ¡ˆ

  Args:
      file_patterns: æª”æ¡ˆè·¯å¾‘æˆ–æ¨¡å¼åˆ—è¡¨
      output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
  """
  try:
    # æ”¶é›†æ‰€æœ‰æª”æ¡ˆ
    all_files = []
    for pattern in file_patterns:
      if os.path.isfile(pattern):
        all_files.append(pattern)
      else:
        matched_files = glob.glob(pattern)
        all_files.extend(matched_files)

    all_files = sorted(list(set(all_files)))

    # åˆ†ææ¯å€‹æª”æ¡ˆ
    file_results = []
    combined_counter = Counter()
    combined_examples = defaultdict(list)

    for file_path in all_files:
      result = analyze_single_file(file_path)
      file_results.append(result)

      if result['success']:
        combined_counter.update(result['word_counter'])

        for word, examples in result['word_examples'].items():
          for example in examples:
            if len(combined_examples[word]) < 5:
              example['file'] = os.path.basename(file_path)
              combined_examples[word].append(example)

    # æ ¼å¼åŒ–çµæœ
    results = {
      'analysis_summary': {
        'total_files_analyzed': len([r for r in file_results if r['success']]),
        'total_files_attempted': len(file_results),
        'total_entries': sum(
            r['total_entries'] for r in file_results if r['success']),
        'total_valid_entries': sum(
            r['valid_entries'] for r in file_results if r['success']),
        'unique_first_words': len(combined_counter),
        'most_common': combined_counter.most_common(1)[
          0] if combined_counter else None
      },
      'file_details': [],
      'combined_word_statistics': []
    }

    # å€‹åˆ¥æª”æ¡ˆè©³æƒ…
    for result in file_results:
      file_detail = {
        'file_path': result['file_path'],
        'file_name': os.path.basename(result['file_path']),
        'success': result['success'],
        'total_entries': result.get('total_entries', 0),
        'valid_entries': result.get('valid_entries', 0),
        'unique_words': len(result['word_counter']),
      }

      if result['success']:
        top_words = result['word_counter'].most_common(5)
        file_detail['top_words'] = [{'word': w, 'count': c} for w, c in
                                    top_words]
      else:
        file_detail['error'] = result.get('error', 'Unknown error')

      results['file_details'].append(file_detail)

    # åˆä½µçµ±è¨ˆ
    total_valid = sum(r['valid_entries'] for r in file_results if r['success'])
    for word, count in combined_counter.most_common():
      # è¨ˆç®—å‡ºç¾åœ¨å¹¾å€‹æª”æ¡ˆä¸­
      files_with_word = []
      for result in file_results:
        if result['success'] and word in result['word_counter']:
          files_with_word.append({
            'file': os.path.basename(result['file_path']),
            'count': result['word_counter'][word]
          })

      word_stat = {
        'word': word,
        'total_count': count,
        'percentage': round((count / total_valid) * 100,
                            2) if total_valid > 0 else 0,
        'appears_in_files': len(files_with_word),
        'file_distribution': files_with_word,
        'examples': combined_examples[word][:3]  # æœ€å¤š3å€‹ç¯„ä¾‹
      }
      results['combined_word_statistics'].append(word_stat)

    # å„²å­˜çµæœ
    if output_file is None:
      output_file = 'multi_file_first_words_analysis.json'

    with open(output_file, 'w', encoding='utf-8') as f:
      json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"âœ… å¤šæª”æ¡ˆåˆ†æçµæœå·²åŒ¯å‡ºè‡³: {output_file}")

  except Exception as e:
    print(f"âŒ åŒ¯å‡ºå¤±æ•—: {str(e)}")


def main():
  """ä¸»å‡½æ•¸"""
  parser = argparse.ArgumentParser(
      description='çµ±è¨ˆå¤šå€‹ JSON è³‡æ–™æª”æ¡ˆä¸­ output æ¬„ä½çš„ç¬¬ä¸€å€‹å­—',
      formatter_class=argparse.RawDescriptionHelpFormatter,
      epilog="""
ç¯„ä¾‹ç”¨æ³•:
  python script.py file1.json file2.json                    # åˆ†ææŒ‡å®šæª”æ¡ˆ
  python script.py "data/*.json"                           # ä½¿ç”¨è¬ç”¨å­—ç¬¦
  python script.py file*.json --min-count 5                # è¨­å®šæœ€å°å‡ºç¾æ¬¡æ•¸
  python script.py "*.json" --export results.json          # åŒ¯å‡ºçµæœ
  python script.py data1.json data2.json --no-per-file     # ä¸é¡¯ç¤ºå€‹åˆ¥æª”æ¡ˆçµ±è¨ˆ
        """)

  parser.add_argument('input_files', nargs='+',
                      help='è¼¸å…¥çš„ JSON æª”æ¡ˆè·¯å¾‘ï¼ˆæ”¯æ´è¬ç”¨å­—ç¬¦ï¼‰')
  parser.add_argument('--no-examples', action='store_true',
                      help='ä¸é¡¯ç¤ºè©³ç´°ç¯„ä¾‹')
  parser.add_argument('--no-per-file', action='store_true',
                      help='ä¸é¡¯ç¤ºå€‹åˆ¥æª”æ¡ˆçµ±è¨ˆ')
  parser.add_argument('--min-count', type=int, default=1,
                      help='æœ€å°å‡ºç¾æ¬¡æ•¸éæ¿¾ (é è¨­: 1)')
  parser.add_argument('--export', help='åŒ¯å‡ºè©³ç´°çµæœåˆ°æŒ‡å®šæª”æ¡ˆ')

  args = parser.parse_args()

  # åŸ·è¡Œåˆ†æ
  analyze_multiple_files(
      args.input_files,
      show_examples=not args.no_examples,
      min_count=args.min_count,
      show_per_file=not args.no_per_file
  )

  # åŒ¯å‡ºçµæœï¼ˆå¦‚æœæŒ‡å®šï¼‰
  if args.export:
    export_multiple_results(args.input_files, args.export)


if __name__ == "__main__":
  main()