import json
import sys
import numpy as np
from pathlib import Path
from transformers import RobertaTokenizer


def analyze_length_distribution(json_file):
  """åˆ†æé•·åº¦åˆ†ä½ˆä¸¦æä¾›å»ºè­°"""

  # è¼‰å…¥ tokenizer
  print("ğŸ”„ è¼‰å…¥ tokenizer...")
  try:
    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')
  except Exception as e:
    print(f"âŒ ç„¡æ³•è¼‰å…¥ tokenizer: {e}")
    return None

  # è®€å–è³‡æ–™
  print(f"ğŸ“– è®€å–æª”æ¡ˆ: {json_file}")
  with open(json_file, 'r', encoding='utf-8') as f:
    data = json.load(f)

  if not isinstance(data, list):
    data = [data]

  # è¨ˆç®—é•·åº¦
  input_lengths = []
  output_lengths = []

  print(f"ğŸ“Š åˆ†æ {len(data):,} ç­†è¨˜éŒ„...")

  for item in data:
    if not isinstance(item,
                      dict) or 'input' not in item or 'output' not in item:
      continue

    input_len = len(tokenizer.encode(item['input'], add_special_tokens=True,
                                     truncation=False))
    output_len = len(tokenizer.encode(item['output'], add_special_tokens=True,
                                      truncation=False))

    input_lengths.append(input_len)
    output_lengths.append(output_len)

  # è¨ˆç®—çµ±è¨ˆ
  def get_stats(lengths):
    return {
      'min': int(np.min(lengths)),
      'max': int(np.max(lengths)),
      'mean': int(np.mean(lengths)),
      'median': int(np.median(lengths)),
      'p90': int(np.percentile(lengths, 90)),
      'p95': int(np.percentile(lengths, 95))
    }

  input_stats = get_stats(input_lengths)
  output_stats = get_stats(output_lengths)

  # é¡¯ç¤ºçµ±è¨ˆ
  print(f"\n{'=' * 40}")
  print("ğŸ“Š é•·åº¦çµ±è¨ˆ")
  print(f"{'=' * 40}")
  print(f"{'æŒ‡æ¨™':<8} {'INPUT':<8} {'OUTPUT':<8}")
  print(f"{'-' * 24}")
  print(f"{'ç¸½æ•¸':<8} {len(input_lengths):<8} {len(output_lengths):<8}")
  print(f"{'æœ€å°':<8} {input_stats['min']:<8} {output_stats['min']:<8}")
  print(f"{'å¹³å‡':<8} {input_stats['mean']:<8} {output_stats['mean']:<8}")
  print(f"{'ä¸­ä½æ•¸':<8} {input_stats['median']:<8} {output_stats['median']:<8}")
  print(f"{'90%':<8} {input_stats['p90']:<8} {output_stats['p90']:<8}")
  print(f"{'95%':<8} {input_stats['p95']:<8} {output_stats['p95']:<8}")
  print(f"{'æœ€å¤§':<8} {input_stats['max']:<8} {output_stats['max']:<8}")

  # æ¨è–¦è¨­å®š
  print(f"\nğŸ’¡ æ¨è–¦è¨­å®š:")

  # è¨ˆç®—ä¸åŒè¨­å®šçš„ä¿ç•™ç‡
  settings = [
    ("å¿«é€Ÿ", 1200, 32),
    ("å¹³è¡¡", 1600, 64),
    ("å®Œæ•´", 2400, 96)
  ]

  for name, input_limit, output_limit in settings:
    kept = sum(1 for i, o in zip(input_lengths, output_lengths)
               if i <= input_limit and o <= output_limit)
    ratio = kept / len(input_lengths) * 100

    print(
      f"  {name}: --max-input {input_limit} --max-output {output_limit} (ä¿ç•™ {ratio:.1f}%)")

  return input_stats, output_stats


def main():
  if len(sys.argv) != 2:
    print("ä½¿ç”¨æ–¹æ³•: python analyze.py <json_file>")
    sys.exit(1)

  json_file = sys.argv[1]

  if not Path(json_file).exists():
    print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {json_file}")
    sys.exit(1)

  result = analyze_length_distribution(json_file)

  if result:
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
  else:
    print(f"\nâŒ åˆ†æå¤±æ•—ï¼")
    sys.exit(1)


if __name__ == "__main__":
  main()