import json
import sys
import numpy as np
from pathlib import Path
from transformers import RobertaTokenizer


def analyze_length_distribution(json_file, input_field='input',
    output_field='output'):
  """åˆ†æé•·åº¦åˆ†ä½ˆä¸¦æä¾›å¯¦ç”¨å»ºè­°"""

  # è¼‰å…¥ tokenizer
  print("è¼‰å…¥ tokenizer...")
  try:
    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')
  except Exception as e:
    print(f"éŒ¯èª¤: ç„¡æ³•è¼‰å…¥ tokenizer: {e}")
    return None

  # è®€å–è³‡æ–™
  print(f"è®€å–æª”æ¡ˆ: {json_file}")
  with open(json_file, 'r', encoding='utf-8') as f:
    data = json.load(f)

  if not isinstance(data, list):
    data = [data]

  # è¨ˆç®—é•·åº¦
  input_lengths = []
  output_lengths = []

  print(f"åˆ†æ {len(data)} ç­†è¨˜éŒ„...")

  for i, item in enumerate(data):
    if not isinstance(item,
                      dict) or input_field not in item or output_field not in item:
      continue

    input_text = item[input_field]
    output_text = item[output_field]

    # ä½¿ç”¨èˆ‡è¨“ç·´ç›¸åŒçš„ tokenization æ–¹å¼
    input_len = len(tokenizer.encode(input_text, add_special_tokens=True,
                                     max_length=10000, truncation=False))
    output_len = len(tokenizer.encode(output_text, add_special_tokens=True,
                                      max_length=10000, truncation=False))

    input_lengths.append(input_len)
    output_lengths.append(output_len)

    # ç°¡åŒ–é€²åº¦é¡¯ç¤º
    if (i + 1) % 2000 == 0:
      print(f"  å·²è™•ç† {i + 1} ç­†...")

  # æ ¸å¿ƒçµ±è¨ˆ
  input_stats = {
    'count': len(input_lengths),
    'min': int(np.min(input_lengths)),
    'max': int(np.max(input_lengths)),
    'mean': int(np.mean(input_lengths)),
    'median': int(np.median(input_lengths)),
    'p75': int(np.percentile(input_lengths, 75)),
    'p90': int(np.percentile(input_lengths, 90)),
    'p95': int(np.percentile(input_lengths, 95))
  }

  output_stats = {
    'count': len(output_lengths),
    'min': int(np.min(output_lengths)),
    'max': int(np.max(output_lengths)),
    'mean': int(np.mean(output_lengths)),
    'median': int(np.median(output_lengths)),
    'p75': int(np.percentile(output_lengths, 75)),
    'p90': int(np.percentile(output_lengths, 90)),
    'p95': int(np.percentile(output_lengths, 95))
  }

  # é¡¯ç¤ºæ ¸å¿ƒçµ±è¨ˆ
  print(f"\n{'=' * 60}")
  print("ğŸ“Š é•·åº¦åˆ†ä½ˆçµ±è¨ˆ")
  print(f"{'=' * 60}")
  print(f"{'æŒ‡æ¨™':<12} {'INPUT':<8} {'OUTPUT':<8}")
  print(f"{'-' * 28}")
  print(f"{'è¨˜éŒ„æ•¸':<12} {input_stats['count']:<8} {output_stats['count']:<8}")
  print(f"{'æœ€å°å€¼':<12} {input_stats['min']:<8} {output_stats['min']:<8}")
  print(
    f"{'ä¸­ä½æ•¸':<12} {input_stats['median']:<8} {output_stats['median']:<8}")
  print(f"{'å¹³å‡å€¼':<12} {input_stats['mean']:<8} {output_stats['mean']:<8}")
  print(f"{'75%':<12} {input_stats['p75']:<8} {output_stats['p75']:<8}")
  print(f"{'90%':<12} {input_stats['p90']:<8} {output_stats['p90']:<8}")
  print(f"{'95%':<12} {input_stats['p95']:<8} {output_stats['p95']:<8}")
  print(f"{'æœ€å¤§å€¼':<12} {input_stats['max']:<8} {output_stats['max']:<8}")

  # å¯¦ç”¨å»ºè­° (åŸºæ–¼Git diffçš„å¯¦éš›ç‰¹æ€§)
  print(f"\n{'=' * 60}")
  print("ğŸ’¡ æ¨è–¦è¨­å®š")
  print(f"{'=' * 60}")

  # é‡å°ä¸åŒä½¿ç”¨å ´æ™¯çš„å»ºè­°
  scenarios = [
    {
      'name': 'ğŸ¯ æ¨è–¦è¨­å®š (æœ€ä½³å¹³è¡¡)',
      'input_limit': 1600,  # ç•¥ä½æ–¼75%ç™¾åˆ†ä½ï¼Œå¹³è¡¡æ•ˆæœå’Œæ•ˆç‡
      'output_limit': 80,  # ç•¥é«˜æ–¼90%ç™¾åˆ†ä½ï¼Œæ¶µè“‹å¤§éƒ¨åˆ†commit
      'description': 'å¹³è¡¡è³‡æ–™ä¿ç•™å’Œè¨“ç·´æ•ˆç‡'
    },
    {
      'name': 'âš¡ é«˜æ•ˆè¨­å®š (å¿«é€Ÿè¨“ç·´)',
      'input_limit': 1200,  # æ¥è¿‘ä¸­ä½æ•¸ï¼Œè¨“ç·´å¿«é€Ÿ
      'output_limit': 50,  # æ¥è¿‘75%ç™¾åˆ†ä½
      'description': 'çŠ§ç‰²éƒ¨åˆ†è³‡æ–™æ›å–è¨“ç·´é€Ÿåº¦'
    },
    {
      'name': 'ğŸ”§ å®Œæ•´è¨­å®š (æœ€å¤§è¦†è“‹)',
      'input_limit': 2400,  # æ¥è¿‘90%ç™¾åˆ†ä½
      'output_limit': 120,  # æ¥è¿‘95%ç™¾åˆ†ä½
      'description': 'ä¿ç•™æœ€å¤šè³‡æ–™ï¼Œéœ€è¦æ›´å¤šè³‡æº'
    }
  ]

  for scenario in scenarios:
    input_limit = scenario['input_limit']
    output_limit = scenario['output_limit']

    # è¨ˆç®—å¯¦éš›ä¿ç•™æ¯”ä¾‹
    kept_both = sum(1 for i, o in zip(input_lengths, output_lengths)
                    if i <= input_limit and o <= output_limit)
    keep_ratio = kept_both / len(input_lengths) * 100

    print(f"\n{scenario['name']}:")
    print(f"  {scenario['description']}")
    print(f"  --max-input {input_limit} --max-output {output_limit}")
    print(
      f"  ä¿ç•™è³‡æ–™: {keep_ratio:.1f}% ({kept_both:,}/{len(input_lengths):,})")

  # é•·åº¦åˆ†ä½ˆæ¦‚è¦½
  print(f"\n{'=' * 60}")
  print("ğŸ“ˆ é•·åº¦åˆ†ä½ˆæ¦‚è¦½")
  print(f"{'=' * 60}")

  ranges = [
    (0, 500, "çŸ­diff"),
    (501, 1000, "ä¸­ç­‰diff"),
    (1001, 2000, "é•·diff"),
    (2001, float('inf'), "è¶…é•·diff")
  ]

  print(f"{'ç¯„åœ':<12} {'INPUTç­†æ•¸':<10} {'æ¯”ä¾‹':<8}")
  print(f"{'-' * 30}")

  for min_len, max_len, label in ranges:
    count = sum(1 for x in input_lengths if min_len <= x <= max_len)
    pct = count / len(input_lengths) * 100
    print(f"{label:<12} {count:<10,} {pct:>6.1f}%")

  return input_stats, output_stats


if __name__ == "__main__":
  if len(sys.argv) < 2:
    print("ä½¿ç”¨æ–¹æ³•: python analyze_length.py <json_file>")
    print("ç¯„ä¾‹: python analyze_length.py data.json")
    sys.exit(1)

  json_file = sys.argv[1]

  if not Path(json_file).exists():
    print(f"éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {json_file}")
    sys.exit(1)

  result = analyze_length_distribution(json_file)

  if result:
    print(f"\nâœ… åˆ†æå®Œæˆï¼")
    print(f"ğŸ’¡ å»ºè­°å…ˆç”¨æ¨è–¦è¨­å®šéæ¿¾è³‡æ–™ï¼Œå†æ ¹æ“šè¨“ç·´æ•ˆæœèª¿æ•´")
  else:
    print(f"\nâŒ åˆ†æå¤±æ•—ï¼")
    sys.exit(1)