import json
import os
import sys
import re
from collections import Counter
from datetime import datetime
from typing import List, Dict, Tuple


class CommitDataPreprocessor:
  def __init__(self):
    self.stats = {
      'total_samples': 0,
      'valid_samples': 0,
      'removed_empty': 0,
      'removed_too_long': 0,
      'removed_invalid_diff': 0,
      'removed_invalid_message': 0,
      'removed_duplicates': 0,
      'removed_low_frequency': 0
    }

  def clean_diff_content(self, diff_text: str) -> str:
    """æ¸…ç† diff å…§å®¹"""
    if not diff_text:
      return ""

    lines = diff_text.split('\n')
    cleaned_lines = []

    for line in lines:
      # ç§»é™¤éé•·çš„è¡Œï¼ˆå¯èƒ½æ˜¯äºŒé€²åˆ¶æª”æ¡ˆæˆ–å£“ç¸®æª”æ¡ˆï¼‰
      if len(line) > 1000:
        continue

      # ä¿ç•™é‡è¦çš„ diff æ¨™è¨˜
      if any(line.startswith(marker) for marker in
             ['diff --git', '@@', '---', '+++', '+', '-', ' ']):
        cleaned_lines.append(line)
      # ä¿ç•™ index è¡Œ
      elif line.startswith('index '):
        cleaned_lines.append(line)
      # ç§»é™¤å…¶ä»–å…ƒæ•¸æ“š
      else:
        continue

    return '\n'.join(cleaned_lines)

  def is_valid_diff(self, diff_text: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ diff"""
    if not diff_text or len(diff_text.strip()) < 10:
      return False

    # å¿…é ˆåŒ…å« diff æ¨™è¨˜
    required_markers = ['diff --git', '@@']
    has_required = any(marker in diff_text for marker in required_markers)

    # å¿…é ˆæœ‰å¯¦éš›çš„ç¨‹å¼ç¢¼è®Šæ›´
    change_lines = [
      line for line in diff_text.split('\n')
      if line.strip() and line.startswith(('+', '-')) and not line.startswith(
          ('+++', '---'))
    ]

    if not change_lines:
      return False

    # éæ¿¾æ‰åªæœ‰å¾®å°è®Šæ›´çš„ diffï¼ˆå¦‚ç©ºè¡Œã€è¨»é‡‹èª¿æ•´ï¼‰
    meaningful_changes = []
    for line in change_lines:
      content = line[1:].strip()  # ç§»é™¤ +/- ç¬¦è™Ÿ

      # è·³éç©ºè¡Œæˆ–åªæœ‰ç¬¦è™Ÿçš„è¡Œ
      if not content or content in ['*', '//', '/*', '*/', '{', '}']:
        continue

      # è·³éåªæœ‰ç©ºç™½å­—ç¬¦èª¿æ•´çš„è¡Œ
      if len(content.replace(' ', '').replace('\t', '')) == 0:
        continue

      meaningful_changes.append(line)

    # è‡³å°‘è¦æœ‰ 2 è¡Œæœ‰æ„ç¾©çš„è®Šæ›´
    return len(meaningful_changes) >= 2

  def clean_commit_message(self, message: str) -> str:
    """æ¸…ç† commit message"""
    if not message:
      return ""

    # åŸºæœ¬æ¸…ç†
    message = message.strip()

    # ç§»é™¤å¤šé¤˜çš„ç©ºè¡Œ
    message = re.sub(r'\n\s*\n', '\n', message)

    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    message = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', message)

    # åªä¿ç•™ç¬¬ä¸€è¡Œï¼ˆé€šå¸¸æ˜¯ä¸»è¦çš„ commit messageï¼‰
    first_line = message.split('\n')[0].strip()

    return first_line

  def is_valid_commit_message(self, message: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦ç‚ºæœ‰æ•ˆçš„ commit message"""
    if not message or len(message.strip()) < 3:
      return False

    # é•·åº¦æª¢æŸ¥
    if len(message) > 200:
      return False

    # æª¢æŸ¥æ˜¯å¦åªæœ‰ä¸€å€‹å­—ä¸²ï¼ˆæ–°å¢çš„æ¢ä»¶ï¼‰
    words = message.strip().split()
    if len(words) <= 1:
      return False

    # éæ¿¾å¸¸è¦‹çš„ç„¡æ„ç¾©è©å½™
    meaningless_words = {
      'polish', 'cleanup', 'fix', 'update', 'change', 'modify',
      'refactor', 'improve', 'enhancement', 'tweaks', 'adjustments'
    }

    # å¦‚æœåªæœ‰ 2 å€‹è©ä¸”ç¬¬ä¸€å€‹è©æ˜¯ç„¡æ„ç¾©çš„ï¼Œå‰‡éæ¿¾æ‰
    if len(words) == 2 and words[0].lower() in meaningless_words:
      return False

    # ä¸æ‡‰è©²åŒ…å« diff ç›¸é—œå…§å®¹
    invalid_patterns = [
      r'diff --git',
      r'@@.*@@',
      r'^\+\+\+',
      r'^---',
      r'^index [a-f0-9]+',
    ]

    for pattern in invalid_patterns:
      if re.search(pattern, message, re.MULTILINE):
        return False

    # æ‡‰è©²åŒ…å«è‹±æ–‡å­—æ¯
    if not re.search(r'[a-zA-Z]', message):
      return False

    return True

  def filter_by_commit_frequency(self, data: List[Dict],
      min_frequency: float = 0.01) -> List[Dict]:
    """æ ¹æ“š commit message ç¬¬ä¸€å€‹è©çš„é »ç‡éæ¿¾"""
    print(f"\nğŸ” åˆ†æ commit message ç¬¬ä¸€å€‹è©çš„é »ç‡...")

    # çµ±è¨ˆç¬¬ä¸€å€‹è©çš„é »ç‡
    first_words = []
    for item in data:
      message = item.get('output', '').strip()
      if message:
        words = message.split()
        if words:
          first_words.append(words[0].lower())

    word_counter = Counter(first_words)
    total_count = len(first_words)

    # é¡¯ç¤ºçµ±è¨ˆ
    print(f"ğŸ“Š ç™¼ç¾ {len(word_counter)} å€‹ä¸åŒçš„é–‹é ­è©")
    print("ğŸ” å‰10å€‹æœ€å¸¸è¦‹çš„é–‹é ­è©:")
    for word, count in word_counter.most_common(10):
      ratio = count / total_count
      print(f"  '{word}': {count} æ¬¡ ({ratio:.3f} = {ratio * 100:.1f}%)")

    # æ‰¾å‡ºé«˜é »è©
    keep_words = set()
    remove_words = set()

    for word, count in word_counter.items():
      ratio = count / total_count
      if ratio >= min_frequency:
        keep_words.add(word)
      else:
        remove_words.add(word)

    print(f"\nğŸ“‹ éæ¿¾çµæœ (é–¾å€¼ >= {min_frequency}):")
    print(f"  âœ… ä¿ç•™ {len(keep_words)} å€‹é«˜é »é–‹é ­è©")
    print(f"  âŒ ç§»é™¤ {len(remove_words)} å€‹ä½é »é–‹é ­è©")

    # éæ¿¾æ•¸æ“š
    filtered_data = []
    removed_count = 0

    for item in data:
      message = item.get('output', '').strip()
      if message:
        words = message.split()
        if words and words[0].lower() in keep_words:
          filtered_data.append(item)
        else:
          removed_count += 1
      else:
        removed_count += 1

    self.stats['removed_low_frequency'] = removed_count
    print(
        f"ğŸ“Š é »ç‡éæ¿¾çµæœ: ä¿ç•™ {len(filtered_data)} ç­†ï¼Œç§»é™¤ {removed_count} ç­†")

    return filtered_data

  def remove_duplicates(self, data: List[Dict]) -> List[Dict]:
    """ç§»é™¤é‡è¤‡çš„è³‡æ–™"""
    print("\nğŸ” ç§»é™¤é‡è¤‡è³‡æ–™...")

    seen_pairs = set()
    unique_data = []
    duplicate_count = 0

    for item in data:
      # ä½¿ç”¨ input å’Œ output çš„çµ„åˆä½œç‚ºå”¯ä¸€æ¨™è­˜
      input_text = item.get('input', '').strip()
      output_text = item.get('output', '').strip()

      # å‰µå»ºç°¡åŒ–çš„æŒ‡ç´‹
      input_fingerprint = input_text[:100] if len(
          input_text) > 100 else input_text
      output_fingerprint = output_text

      pair_key = (input_fingerprint, output_fingerprint)

      if pair_key not in seen_pairs:
        seen_pairs.add(pair_key)
        unique_data.append(item)
      else:
        duplicate_count += 1

    self.stats['removed_duplicates'] = duplicate_count
    print(
        f"ğŸ“Š é‡è¤‡æª¢æŸ¥çµæœ: ä¿ç•™ {len(unique_data)} ç­†ï¼Œç§»é™¤ {duplicate_count} ç­†é‡è¤‡")

    return unique_data

  def preprocess_data(self, data_path: str, output_path: str = None,
      max_input_length: int = 2000, max_output_length: int = 100,
      min_frequency: float = 0.01) -> str:
    """å®Œæ•´çš„è³‡æ–™é è™•ç†æµç¨‹"""

    print("ğŸš€ é–‹å§‹ Commit Message è³‡æ–™é è™•ç†")
    print("=" * 60)

    # è¼‰å…¥è³‡æ–™
    print(f"ğŸ“ è¼‰å…¥è³‡æ–™: {data_path}")
    with open(data_path, 'r', encoding='utf-8') as f:
      data = json.load(f)

    self.stats['total_samples'] = len(data)
    print(f"ğŸ“Š åŸå§‹è³‡æ–™: {len(data)} ç­†")

    # æ­¥é©Ÿ1: åŸºæœ¬æ¸…ç†å’Œé©—è­‰
    print(f"\nğŸ§¹ æ­¥é©Ÿ1: åŸºæœ¬æ¸…ç†å’Œé©—è­‰")
    cleaned_data = []

    for i, item in enumerate(data):
      if not isinstance(item, dict):
        continue

      if 'input' not in item or 'output' not in item:
        continue

      # æ¸…ç† diff
      clean_input = self.clean_diff_content(str(item['input']))
      clean_output = self.clean_commit_message(str(item['output']))

      # åŸºæœ¬é©—è­‰
      if not clean_input or not clean_output:
        self.stats['removed_empty'] += 1
        continue

      # é•·åº¦æª¢æŸ¥
      if len(clean_input) > max_input_length or len(
          clean_output) > max_output_length:
        self.stats['removed_too_long'] += 1
        continue

      # æ ¼å¼é©—è­‰
      if not self.is_valid_diff(clean_input):
        self.stats['removed_invalid_diff'] += 1
        continue

      if not self.is_valid_commit_message(clean_output):
        self.stats['removed_invalid_message'] += 1
        continue

      cleaned_data.append({
        'input': clean_input,
        'output': clean_output
      })

    print(f"ğŸ“Š åŸºæœ¬æ¸…ç†çµæœ: {len(cleaned_data)} ç­†æœ‰æ•ˆè³‡æ–™")

    # æ­¥é©Ÿ2: ç§»é™¤é‡è¤‡
    print(f"\nğŸ” æ­¥é©Ÿ2: ç§»é™¤é‡è¤‡è³‡æ–™")
    unique_data = self.remove_duplicates(cleaned_data)

    # æ­¥é©Ÿ3: é »ç‡éæ¿¾
    print(f"\nğŸ“Š æ­¥é©Ÿ3: æ ¹æ“š commit message é–‹é ­è©é »ç‡éæ¿¾")
    filtered_data = self.filter_by_commit_frequency(unique_data, min_frequency)

    self.stats['valid_samples'] = len(filtered_data)

    # ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆå
    if output_path is None:
      base_name = os.path.splitext(data_path)[0]
      timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
      output_path = f"{base_name}_preprocessed_{timestamp}.json"

    # å„²å­˜çµæœ
    with open(output_path, 'w', encoding='utf-8') as f:
      json.dump(filtered_data, f, ensure_ascii=False, indent=2)

    # é¡¯ç¤ºçµ±è¨ˆå ±å‘Š
    self.print_report()
    print(f"\nğŸ’¾ é è™•ç†å®Œæˆï¼")
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {output_path}")
    print(f"ğŸ“Š æœ€çµ‚è³‡æ–™é‡: {len(filtered_data)} ç­†")

    return output_path

  def print_report(self):
    """åˆ—å°é è™•ç†å ±å‘Š"""
    print(f"\nğŸ“‹ é è™•ç†çµ±è¨ˆå ±å‘Š")
    print("=" * 40)
    print(f"ğŸ“Š åŸå§‹è³‡æ–™:     {self.stats['total_samples']:>6} ç­†")
    print(f"âŒ ç§»é™¤ç©ºå€¼:     {self.stats['removed_empty']:>6} ç­†")
    print(f"âŒ ç§»é™¤éé•·:     {self.stats['removed_too_long']:>6} ç­†")
    print(f"âŒ ç„¡æ•ˆ diff:    {self.stats['removed_invalid_diff']:>6} ç­†")
    print(f"âŒ ç„¡æ•ˆ message: {self.stats['removed_invalid_message']:>6} ç­†")
    print(f"âŒ ç§»é™¤é‡è¤‡:     {self.stats['removed_duplicates']:>6} ç­†")
    print(f"âŒ ä½é »éæ¿¾:     {self.stats['removed_low_frequency']:>6} ç­†")
    print(f"âœ… æœ€çµ‚ä¿ç•™:     {self.stats['valid_samples']:>6} ç­†")

    if self.stats['total_samples'] > 0:
      retention_rate = self.stats['valid_samples'] / self.stats[
        'total_samples'] * 100
      print(f"ğŸ“ˆ ä¿ç•™ç‡:       {retention_rate:>5.1f}%")


def main():
  """ä¸»å‡½æ•¸"""
  print("ğŸ› ï¸  CodeT5 Commit Message è³‡æ–™é è™•ç†å·¥å…·")
  print("=" * 60)

  if len(sys.argv) < 2:
    print("âŒ ä½¿ç”¨æ–¹æ³•:")
    print(f"   python {sys.argv[0]} <è¼¸å…¥æª”æ¡ˆè·¯å¾‘> [è¼¸å‡ºæª”æ¡ˆè·¯å¾‘]")
    print("\nç¯„ä¾‹:")
    print(f"   python {sys.argv[0]} spring-boot-training.json")
    print(f"   python {sys.argv[0]} data.json cleaned_data.json")
    return

  input_path = sys.argv[1]
  output_path = sys.argv[2] if len(sys.argv) > 2 else None

  if not os.path.exists(input_path):
    print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {input_path}")
    return

  # å»ºç«‹é è™•ç†å™¨
  preprocessor = CommitDataPreprocessor()

  try:
    # åŸ·è¡Œé è™•ç†
    result_path = preprocessor.preprocess_data(
        data_path=input_path,
        output_path=output_path,
        max_input_length=2000,  # diff æœ€å¤§é•·åº¦
        max_output_length=100,  # commit message æœ€å¤§é•·åº¦
        min_frequency=0.01  # æœ€å°é »ç‡é–¾å€¼ 1%
    )

    print(f"\nğŸ‰ é è™•ç†å®Œæˆï¼")
    print(f"ğŸ’¡ åœ¨è¨“ç·´ç¨‹å¼ä¸­ä½¿ç”¨:")
    print(f"   data_path = \"{result_path}\"")

  except Exception as e:
    print(f"âŒ é è™•ç†å¤±æ•—: {e}")
    raise e


if __name__ == "__main__":
  main()