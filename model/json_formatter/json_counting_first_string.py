import json
import re
import argparse
from collections import Counter
from typing import Dict, List, Any, Tuple


def extract_first_word(text: str) -> str:
  """
  æå–æ–‡æœ¬çš„ç¬¬ä¸€å€‹å–®è©

  Args:
      text: è¼¸å…¥æ–‡æœ¬

  Returns:
      ç¬¬ä¸€å€‹å–®è©ï¼ˆè½‰ç‚ºå°å¯«ï¼‰
  """
  if not text:
    return ""

  # ç§»é™¤é–‹é ­çš„ç©ºç™½å­—ç¬¦
  text = text.strip()

  # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–ç¬¬ä¸€å€‹å–®è©
  # æ”¯æ´è‹±æ–‡å–®è©ã€ä¸­æ–‡å­—ç¬¦ã€æ•¸å­—ç­‰
  match = re.match(r'[\w\u4e00-\u9fff]+', text)

  if match:
    return match.group().lower()
  else:
    # å¦‚æœæ²’æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›ç¬¬ä¸€å€‹éç©ºç™½å­—ç¬¦
    for char in text:
      if not char.isspace():
        return char.lower()
    return ""


def analyze_first_words(file_path: str, show_examples: bool = True,
    min_count: int = 1) -> None:
  """
  åˆ†æ JSON æª”æ¡ˆä¸­ output æ¬„ä½çš„ç¬¬ä¸€å€‹å­—

  Args:
      file_path: JSON æª”æ¡ˆè·¯å¾‘
      show_examples: æ˜¯å¦é¡¯ç¤ºç¯„ä¾‹
      min_count: æœ€å°å‡ºç¾æ¬¡æ•¸éæ¿¾
  """
  try:
    # è®€å– JSON æª”æ¡ˆ
    with open(file_path, 'r', encoding='utf-8') as f:
      data = json.load(f)

    print(f"ğŸ“‚ åˆ†ææª”æ¡ˆ: {file_path}")
    print(f"ğŸ“Š ç¸½è³‡æ–™ç­†æ•¸: {len(data)}")
    print("=" * 60)

    # æ”¶é›†ç¬¬ä¸€å€‹å­—å’Œå°æ‡‰çš„å®Œæ•´æ–‡æœ¬ç¯„ä¾‹
    first_words = []
    word_examples = {}

    for i, item in enumerate(data):
      if 'output' in item and item['output']:
        output_text = item['output']
        first_word = extract_first_word(output_text)

        if first_word:
          first_words.append(first_word)

          # å„²å­˜ç¯„ä¾‹ï¼ˆæ¯å€‹å­—æœ€å¤šä¿å­˜3å€‹ç¯„ä¾‹ï¼‰
          if first_word not in word_examples:
            word_examples[first_word] = []

          if len(word_examples[first_word]) < 3:
            word_examples[first_word].append({
              'index': i + 1,
              'text': output_text
            })

    # çµ±è¨ˆé »ç‡
    word_counter = Counter(first_words)

    # éæ¿¾æœ€å°å‡ºç¾æ¬¡æ•¸
    filtered_words = {word: count for word, count in word_counter.items() if
                      count >= min_count}

    print(
      f"ğŸ”¤ æ‰¾åˆ° {len(filtered_words)} å€‹ä¸åŒçš„ç¬¬ä¸€å€‹å­— (å‡ºç¾æ¬¡æ•¸ >= {min_count})")
    print(f"ğŸ“ˆ ç¸½è¨ˆ {sum(filtered_words.values())} ç­†æœ‰æ•ˆè³‡æ–™")
    print()

    # æŒ‰é »ç‡æ’åºé¡¯ç¤º
    sorted_words = sorted(filtered_words.items(), key=lambda x: (-x[1], x[0]))

    print("ğŸ“‹ ç¬¬ä¸€å€‹å­—çµ±è¨ˆ (æŒ‰é »ç‡æ’åº):")
    print("-" * 60)
    print(f"{'ç¬¬ä¸€å€‹å­—':<15} {'æ¬¡æ•¸':<8} {'ç™¾åˆ†æ¯”':<8} {'ç¯„ä¾‹'}")
    print("-" * 60)

    total_count = sum(filtered_words.values())

    for word, count in sorted_words[:50]:  # é¡¯ç¤ºå‰50å€‹
      percentage = (count / total_count) * 100

      # é¡¯ç¤ºç¬¬ä¸€å€‹ç¯„ä¾‹
      example = ""
      if word in word_examples and word_examples[word]:
        example_text = word_examples[word][0]['text']
        example = example_text[:40] + "..." if len(
          example_text) > 40 else example_text
        example = example.replace('\n', ' ')

      print(f"{word:<15} {count:<8} {percentage:<7.1f}% {example}")

    if len(sorted_words) > 50:
      print(f"\n... é‚„æœ‰ {len(sorted_words) - 50} å€‹ç¬¬ä¸€å€‹å­—")

    # é¡¯ç¤ºè©³ç´°ç¯„ä¾‹
    if show_examples:
      print("\n" + "=" * 60)
      print("ğŸ“ è©³ç´°ç¯„ä¾‹ (å‰10å€‹æœ€å¸¸è¦‹çš„ç¬¬ä¸€å€‹å­—):")
      print("=" * 60)

      for word, count in sorted_words[:10]:
        print(f"\nğŸ”¤ '{word}' (å‡ºç¾ {count} æ¬¡):")

        if word in word_examples:
          for j, example in enumerate(word_examples[word], 1):
            print(f"  ç¯„ä¾‹ {j}: {example['text']}")
        print("-" * 50)

    # çµ±è¨ˆæ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š çµ±è¨ˆæ‘˜è¦:")
    print(
      f"â€¢ æœ€å¸¸è¦‹çš„ç¬¬ä¸€å€‹å­—: '{sorted_words[0][0]}' ({sorted_words[0][1]} æ¬¡)")
    print(f"â€¢ ç¸½å…± {len(word_counter)} å€‹ä¸åŒçš„ç¬¬ä¸€å€‹å­—")
    print(
      f"â€¢ å¹³å‡æ¯å€‹å­—å‡ºç¾ {sum(word_counter.values()) / len(word_counter):.1f} æ¬¡")

    # é¡¯ç¤ºå‡ºç¾æ¬¡æ•¸ç‚º1çš„å­—ï¼ˆå¯èƒ½æ˜¯æ‹¼å¯«éŒ¯èª¤æˆ–ç‰¹æ®Šæƒ…æ³ï¼‰
    singleton_words = [word for word, count in word_counter.items() if
                       count == 1]
    if singleton_words:
      print(f"â€¢ åªå‡ºç¾1æ¬¡çš„å­—: {len(singleton_words)} å€‹")
      if len(singleton_words) <= 20:
        print(f"  â†’ {', '.join(sorted(singleton_words))}")

  except FileNotFoundError:
    print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
  except json.JSONDecodeError:
    print(f"âŒ æª”æ¡ˆä¸æ˜¯æœ‰æ•ˆçš„ JSON: {file_path}")
  except Exception as e:
    print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}")


def export_results(file_path: str, output_file: str = None) -> None:
  """
  å°‡ç¬¬ä¸€å€‹å­—çµ±è¨ˆçµæœåŒ¯å‡ºåˆ° JSON æª”æ¡ˆ

  Args:
      file_path: è¼¸å…¥ JSON æª”æ¡ˆè·¯å¾‘
      output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
  """
  try:
    with open(file_path, 'r', encoding='utf-8') as f:
      data = json.load(f)

    # æ”¶é›†è³‡æ–™
    first_words = []
    word_examples = {}

    for i, item in enumerate(data):
      if 'output' in item and item['output']:
        output_text = item['output']
        first_word = extract_first_word(output_text)

        if first_word:
          first_words.append(first_word)

          if first_word not in word_examples:
            word_examples[first_word] = []

          word_examples[first_word].append({
            'index': i + 1,
            'text': output_text
          })

    # çµ±è¨ˆä¸¦æ ¼å¼åŒ–çµæœ
    word_counter = Counter(first_words)

    results = {
      'summary': {
        'total_entries': len(data),
        'valid_entries': len(first_words),
        'unique_first_words': len(word_counter),
        'most_common': word_counter.most_common(1)[0] if word_counter else None
      },
      'word_statistics': []
    }

    for word, count in word_counter.most_common():
      word_stat = {
        'word': word,
        'count': count,
        'percentage': round((count / len(first_words)) * 100, 2),
        'examples': word_examples[word][:3]  # æœ€å¤š3å€‹ç¯„ä¾‹
      }
      results['word_statistics'].append(word_stat)

    # å„²å­˜çµæœ
    if output_file is None:
      output_file = file_path.replace('.json', '_first_words_analysis.json')

    with open(output_file, 'w', encoding='utf-8') as f:
      json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"âœ… åˆ†æçµæœå·²åŒ¯å‡ºè‡³: {output_file}")

  except Exception as e:
    print(f"âŒ åŒ¯å‡ºå¤±æ•—: {str(e)}")


def main():
  """ä¸»å‡½æ•¸"""
  parser = argparse.ArgumentParser(
    description='çµ±è¨ˆ JSON è³‡æ–™ä¸­ output æ¬„ä½çš„ç¬¬ä¸€å€‹å­—')
  parser.add_argument('input_file', help='è¼¸å…¥çš„ JSON æª”æ¡ˆè·¯å¾‘')
  parser.add_argument('--no-examples', action='store_true',
                      help='ä¸é¡¯ç¤ºè©³ç´°ç¯„ä¾‹')
  parser.add_argument('--min-count', type=int, default=1,
                      help='æœ€å°å‡ºç¾æ¬¡æ•¸éæ¿¾ (é è¨­: 1)')
  parser.add_argument('--export', help='åŒ¯å‡ºè©³ç´°çµæœåˆ°æŒ‡å®šæª”æ¡ˆ')

  args = parser.parse_args()

  # åŸ·è¡Œåˆ†æ
  analyze_first_words(
      args.input_file,
      show_examples=not args.no_examples,
      min_count=args.min_count
  )

  # åŒ¯å‡ºçµæœï¼ˆå¦‚æœæŒ‡å®šï¼‰
  if args.export:
    export_results(args.input_file, args.export)


def quick_analysis(file_path: str) -> Dict[str, int]:
  """
  å¿«é€Ÿåˆ†æï¼Œè¿”å›ç¬¬ä¸€å€‹å­—çš„çµ±è¨ˆå­—å…¸

  Args:
      file_path: JSON æª”æ¡ˆè·¯å¾‘

  Returns:
      {ç¬¬ä¸€å€‹å­—: å‡ºç¾æ¬¡æ•¸} çš„å­—å…¸
  """
  try:
    with open(file_path, 'r', encoding='utf-8') as f:
      data = json.load(f)

    first_words = []
    for item in data:
      if 'output' in item and item['output']:
        first_word = extract_first_word(item['output'])
        if first_word:
          first_words.append(first_word)

    return dict(Counter(first_words))

  except Exception as e:
    print(f"å¿«é€Ÿåˆ†æå¤±æ•—: {str(e)}")
    return {}


if __name__ == "__main__":
  main()