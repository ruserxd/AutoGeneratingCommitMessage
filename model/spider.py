import os
import json
import time
import requests
import re
from datetime import datetime

# ====== ä¿®æ”¹é€™è£¡ ======
GITHUB_TOKEN = ""
MAX_COMMITS_PER_REPO = 50
MAX_SKIP_STREAK = 10
REPO_PAGES = 2       # æœå°‹ repo é æ•¸
REPO_PER_PAGE = 5    # æ¯é  repo æ•¸é‡
# ======================

HEADERS = {"Authorization": f"token {GITHUB_TOKEN}"}
BASE_SPIDER_DIR = os.path.join(os.getcwd(), "spider-data")
os.makedirs(BASE_SPIDER_DIR, exist_ok=True)
seen_sha_path = os.path.join(BASE_SPIDER_DIR, "seen_commits.json")

# è®€å–å·²è™•ç†éçš„ commit SHA
def load_seen_shas():
    if os.path.exists(seen_sha_path):
        with open(seen_sha_path, "r", encoding="utf-8") as f:
            return set(json.load(f))
    return set()

# å„²å­˜ seen SHA
def save_seen_shas(seen_shas):
    with open(seen_sha_path, "w", encoding="utf-8") as f:
        json.dump(list(seen_shas), f, ensure_ascii=False, indent=2)

def search_java_repos(pages=1, per_page=5):
    repos = []
    for page in range(1, pages + 1):
        url = "https://api.github.com/search/repositories"
        params = {
            "q": "language:Java",
            "sort": "stars",
            "order": "desc",
            "per_page": per_page,
            "page": page
        }
        r = requests.get(url, headers=HEADERS, params=params)
        r.raise_for_status()
        data = r.json()
        for item in data["items"]:
            repos.append((item["owner"]["login"], item["name"]))
        time.sleep(1)
    return repos

def get_commits(owner, repo, per_page=30, page=1):
    url = f"https://api.github.com/repos/{owner}/{repo}/commits"
    params = {"per_page": per_page, "page": page}
    r = requests.get(url, headers=HEADERS, params=params)
    if r.status_code == 409:
        return []  # ç©º repo
    r.raise_for_status()
    return r.json()

def get_commit_detail(owner, repo, sha):
    url = f"https://api.github.com/repos/{owner}/{repo}/commits/{sha}"
    r = requests.get(url, headers=HEADERS)
    r.raise_for_status()
    return r.json()

def extract_java_patch(commit_data):
    message = commit_data["commit"]["message"]

    # 1ï¸âƒ£ å‰”é™¤å«ä¸­æ–‡ commit message
    if re.search(r'[\u4e00-\u9fff]', message):
        return None

    files = commit_data.get("files", [])
    java_patches = []

    for f in files:
        filename = f.get("filename", "")
        patch = f.get("patch", "")
        if filename.endswith(".java") and patch:
            # 2ï¸âƒ£ æª¢æŸ¥æ˜¯å¦åªæ”¹è¨»è§£
            lines = patch.splitlines()
            added_lines = [line[1:].strip() for line in lines if line.startswith("+") and not line.startswith("+++")]
            total = len(added_lines)
            comment_like = sum(
                1 for line in added_lines
                if line.startswith("//") or line.startswith("*") or line.startswith("/*") or line.startswith("*/")
            )
            if total > 0 and comment_like / total > 0.8:
                return None  # éæ¿¾åªæ”¹è¨»è§£çš„ commit

            # 3ï¸âƒ£ è£œä¸Šå®Œæ•´ Git diff æ ¼å¼
            diff_header = f"diff --git a/{filename} b/{filename}\n"
            file_header = f"--- a/{filename}\n+++ b/{filename}\n"
            full_patch = diff_header + file_header + patch
            java_patches.append(full_patch)

    if not java_patches:
        return None

    return {
        "input": "\n\n".join(java_patches),
        "output": message.strip()
    }
def crawl_repo_commits(owner, repo, seen_shas, max_commits=50, max_skip_streak=10, max_too_long=10):
    print(f"\nğŸ“¦ é–‹å§‹æŠ“å–ï¼š{owner}/{repo}")
    results = []
    page = 1
    skip_streak = 0
    too_long_count = 0

    while len(results) < max_commits:
        try:
            commits = get_commits(owner, repo, page=page)
            if not commits:
                break

            for commit in commits:
                if len(results) >= max_commits:
                    break
                sha = commit["sha"]
                if sha in seen_shas:
                    print(f"  ğŸ” Commit {sha[:7]} å·²å­˜åœ¨ï¼Œè·³é")
                    continue

                try:
                    detail = get_commit_detail(owner, repo, sha)
                    data = extract_java_patch(detail)
                    if data:
                        # æª¢æŸ¥ input é•·åº¦
                        if len(data["input"]) > 7000:
                            too_long_count += 1
                            print(f"  ğŸš« Commit {sha[:7]} éé•·è·³éï¼ˆ{too_long_count}/{max_too_long}ï¼‰")
                            if too_long_count >= max_too_long:
                                print(f"  âŒ è¶…éå…è¨±çš„éé•· commit æ¬¡æ•¸ï¼Œåœæ­¢æ­¤ repo")
                                return results
                            continue

                        results.append(data)
                        seen_shas.add(sha)
                        skip_streak = 0
                        print(f"  âœ… Commit {sha[:7]} saved ({len(results)}/{max_commits})")
                    else:
                        skip_streak += 1
                        print(f"  â­ï¸ Commit {sha[:7]} skippedï¼ˆç„¡ .java patchï¼‰({skip_streak}/{max_skip_streak})")
                        if skip_streak >= max_skip_streak:
                            print(f"  âŒ è¶…éå…è¨±çš„ç„¡ Java commit æ¬¡æ•¸ï¼Œåœæ­¢æ­¤ repo")
                            return results

                    time.sleep(0.8)

                except Exception as e:
                    print(f"  âš ï¸ Commit {sha[:7]} error: {e}")
            page += 1

        except Exception as e:
            print(f"  âŒ Error fetching commits: {e}")
            break

    return results


def main():
    print("ğŸš€ é–‹å§‹æœå°‹ç†±é–€ Java å°ˆæ¡ˆ...")
    seen_shas = load_seen_shas()
    repo_list = search_java_repos(pages=10, per_page=5)  # æœå°‹ 50 å€‹ repoï¼ˆè¦–æƒ…æ³èª¿æ•´ï¼‰

    all_results = []
    TARGET_TOTAL_RESULTS = 50  # âœ… ä½ è¦æŠ“åˆ°çš„ç¸½è³‡æ–™é‡

    for owner, repo in repo_list:
        if len(all_results) >= TARGET_TOTAL_RESULTS:
            break

        repo_results = crawl_repo_commits(
            owner, repo,
            seen_shas,
            max_commits=MAX_COMMITS_PER_REPO,
            max_skip_streak=5,
            max_too_long=5
        )

        if repo_results:
            all_results.extend(repo_results)
        time.sleep(1)

    # å„²å­˜æ–°çµæœ
    if all_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(BASE_SPIDER_DIR, f"all_java_commits_{timestamp}.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… å…± {len(all_results)} ç­†æ–°è³‡æ–™å·²å„²å­˜ï¼š{output_path}")
    else:
        print("âš ï¸ æœ¬æ¬¡ç„¡æœ‰æ•ˆè³‡æ–™")

    # å„²å­˜ SHA æ¸…å–®
    save_seen_shas(seen_shas)
    print(f"ğŸ“Œ å·²æ›´æ–° seen_commits.jsonï¼ˆå…± {len(seen_shas)} ç­† SHAï¼‰")


if __name__ == "__main__":
    main()



